{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a783cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "import time\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb98d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory from where the Training, Validation and Test datasets are saved\n",
    "\n",
    "train_path1 = ''\n",
    "train_path2 = ''\n",
    "train_path3 = ''\n",
    "train_path4 = ''\n",
    "train_path5 = ''\n",
    "train_path6 = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_path1 = ''\n",
    "valid_path2 = ''\n",
    "valid_path3 = ''\n",
    "valid_path4 = ''\n",
    "valid_path5 = ''\n",
    "valid_path6 = ''\n",
    "\n",
    "\n",
    "\n",
    "test_path1 = ''\n",
    "test_path2 = ''\n",
    "test_path3 = ''\n",
    "test_path4 = ''\n",
    "test_path5 = ''\n",
    "test_path6 = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0181a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ImageDataGenerator to create the random batches of specklegram image datasets in which \n",
    "# training (train_batches) and validation (validation_batches) datasets are shuffled \n",
    "# and then the CNN model is tested on test (test_batches) datasets.  \n",
    "\n",
    "train_batches1 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=train_path1, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "train_batches2 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=train_path2, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "train_batches3 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=train_path3, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "train_batches4 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=train_path4, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "train_batches5 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=train_path5, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "train_batches6 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=train_path6, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "\n",
    "\n",
    "valid_batches1 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=valid_path1, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1) \n",
    "valid_batches2 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=valid_path2, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "valid_batches3 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=valid_path3, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "valid_batches4 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=valid_path4, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "valid_batches5 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=valid_path5, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "valid_batches6 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=valid_path6, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1)\n",
    "\n",
    "\n",
    "test_batches1 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=test_path1, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1,shuffle=False)\n",
    "test_batches2 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=test_path2, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1,shuffle=False)\n",
    "test_batches3 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=test_path3, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1,shuffle=False)\n",
    "test_batches4 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=test_path4, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1,shuffle=False)\n",
    "test_batches5 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=test_path5, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1,shuffle=False)\n",
    "test_batches6 = ImageDataGenerator(rescale=1/255).flow_from_directory(directory=test_path6, target_size=(400,400),classes=['1','2','3','4','5','6','7'],batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f661fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the label values and their corresponding \"one-hot encodings\"\n",
    "\n",
    "train_batches1.class_indices\n",
    "train_batches1.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the frozen weights pre-trained VGG-16 model and save into the variable vgg \n",
    "\n",
    "vgg = VGG16(input_shape = (img_size,img_size,3), weights='imagenet',include_top=False)\n",
    "\n",
    "vgg.trainable = False \n",
    "\n",
    "vgg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a967c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 11, 11, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 247808)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               126878208 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 148,684,583\n",
      "Trainable params: 126,881,799\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the tail end of the model with dense layers and the concatenate with the VGG-16 model and define model1 as \n",
    "#the total model.  \n",
    "\n",
    "\n",
    "\n",
    "flatten_layer = keras.layers.Flatten()\n",
    "dense_layer_1 = keras.layers.Dense(512, activation='relu')\n",
    "dense_layer_2 = keras.layers.Dense(512, activation='relu')\n",
    "dense_layer_3 = keras.layers.Dense(512, activation='relu')\n",
    "dense_layer_4 = keras.layers.Dense(512, activation='relu')\n",
    "dense_layer_5 = keras.layers.Dense(512, activation='relu')\n",
    "dense_layer_6 = keras.layers.Dense(512, activation='relu')\n",
    "dense_layer_7 = keras.layers.Dense(512, activation='relu')\n",
    "prediction_layer = keras.layers.Dense(7, activation='softmax')\n",
    "\n",
    "\n",
    "model1 = keras.Sequential([ vgg, flatten_layer, dense_layer_1,  dense_layer_2,  dense_layer_3,  dense_layer_4,\n",
    "                            dense_layer_5,  dense_layer_6,  dense_layer_7, keras.layers.BatchNormalization(),\n",
    "                            keras.layers.Dropout(0.2), prediction_layer])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8216f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keras optimizer, \"Adam\" for training and validation of the model. Also compile the model with the \n",
    "# classification loss calles as \"Categorically Crossentropy\" and metrics as the \"Accuracy\" of the classification model. \n",
    "\n",
    "opt= keras.optimizers.adam(learning_rate=1e-4)\n",
    "model1.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the execution of the model. \n",
    "\n",
    "start_time = time.time()\n",
    "train_hist1 = model1.fit(train_batches1,validation_data=valid_batches1,epochs=10,verbose=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c626f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs epochs \n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(train_hist1.history['accuracy'],'r',linewidth=3)\n",
    "#plt.plot(valid_hist1.history['accuracy'],'b',linewidth=3)\n",
    "plt.plot(train_hist1.history['val_accuracy'],'b',linewidth=3)\n",
    "plt.xlabel('Epochs',weight='bold',fontsize=18)\n",
    "plt.ylabel('Accuracy',weight='bold',fontsize=18)\n",
    "legend_properties = {'weight':'bold','size':18}\n",
    "plt.xticks(weight = 'bold',fontsize=18)\n",
    "plt.yticks(weight = 'bold',fontsize=18)\n",
    "plt.legend(['Training', 'Validation'], loc='lower right',prop=legend_properties)\n",
    "plt.show()\n",
    "fig1.savefig('',dpi=600,bbox_inches='tight')\n",
    "\n",
    "# Plot loss vs epochs\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(train_hist1.history['loss'],'r',linewidth=3)\n",
    "#plt.plot(valid_hist1.history['loss'],'b',linewidth=3)\n",
    "plt.plot(train_hist1.history['val_loss'],'b',linewidth=3)\n",
    "plt.xlabel('Epochs',weight='bold',fontsize=18)\n",
    "plt.xticks(weight = 'bold',fontsize=18)\n",
    "plt.yticks(weight = 'bold',fontsize=18)\n",
    "plt.ylabel('Loss',weight='bold',fontsize=18)\n",
    "plt.legend(['Training', 'Validation'], loc='upper right',prop=legend_properties)\n",
    "plt.show()\n",
    "fig1.savefig('',dpi=600,bbox_inches='tight') #'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the model execution for predictions on test dataset and rounding off to nearest integer\n",
    "# and to find the indices of the maximum values (pred1).\n",
    "\n",
    "predictions1 = model1.predict(x=test_batches1)\n",
    "\n",
    "predictions1 = np.uint(np.round(predictions1,1))\n",
    "pred1 = np.argmax(predictions1,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a9157d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet for Conusion matrix plotting.\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "cm = confusion_matrix(pred1,test_batches1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c5c481d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for Confusion matrix plotting. \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.seismic):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    fig1 = plt.figure(figsize=(12,7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "        \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes,weight='bold')\n",
    "    plt.yticks(tick_marks, classes,weight='bold')\n",
    "\n",
    "    if normalize:\n",
    "        cm = np.round((cm.astype('float') / cm.sum(axis=0)[:, np.newaxis])*100,1) \n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "   \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],weight='bold',fontsize=20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"white\")\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Ground truth',fontsize=18,weight='bold')\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel('Model prediction',labelpad=1,fontsize=18,weight='bold')\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.title('Confusion Matrix',weight='bold',fontsize=18)\n",
    "    fig1.savefig('.png',dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels\n",
    "cm_plot_labels = ['', '', '', '','','','']\n",
    "\n",
    "#Plot the confusion matrix. \n",
    "plot_confusion_matrix(cm = cm,classes = cm_plot_labels,title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d690d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c034cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59a461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
